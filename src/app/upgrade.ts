import { Driver, Forager } from 'forager'
import { mkdirp } from 'fs-extra'
import { basename, join, parse, stripProtocol } from 'protopath'

import { Ept, Source } from 'ept'
import { JsonSchema, Pool, ValidationError } from 'utils'

async function readJson(storage: Driver, filename: string) {
  return JSON.parse((await storage.read(filename)).toString())
}

/*
async function readList0(storage: Driver, filename: string) {
  const json = await readJson(storage, filename)
  const [list, errors] = JsonSchema.validate<Source.V0.Summary>(
    Source.V0.summary.schema,
    json
  )
  if (errors.length) {
    throw new ValidationError(
      'Failed to vaildate data source summary v1.0.0',
      errors
    )
  }
  return list
}
*/

type MaybeBackup = {
  dir: string
  threads: number
  verbose: boolean
}
async function maybeBackup({ dir: src, threads, verbose }: MaybeBackup) {
  const dst = join(src, 'ept-backup')
  if (Forager.getProtocolOrDefault(dst) === 'file') {
    await mkdirp(dst)
    await mkdirp(join(dst, 'ept-hierarchy'))
    await mkdirp(join(dst, 'ept-sources'))
  }

  // If our original metadata is already backed up, we're done here.
  {
    const list = await Forager.list(dst, false)
    const exists = Boolean(list.find((v) => v.path === 'ept.json'))
    if (exists) {
      if (verbose) console.log(`Metadata for ${src} already backed up`)
      return
    }
  }

  if (verbose) console.log('Backing up metadata...')

  // Copy the source file metadata.
  const sources = await Forager.list(join(src, 'ept-sources'), true)
  await Pool.all(
    sources.map((v) => async () =>
      Forager.copyFile(
        join(src, 'ept-sources', v.path),
        join(dst, 'ept-sources', v.path)
      )
    ),
    threads
  )

  // And finally, ept.json, which is intentionally the last item since we use it
  // as a sentinel to determine whether our metadata is already backed up.
  await Forager.copyFile(join(src, 'ept.json'), join(dst, 'ept.json'))

  if (verbose) console.log('\tMetadata backup complete')
}

type UpgradeDir = {
  dir: string
  threads?: number
  verbose?: boolean
}
export async function upgradeDir({ dir, threads, verbose }: UpgradeDir) {
  const list = (await Forager.list(dir, false))
    .filter((v) => v.type === 'directory')
    .map((v) => v.path)

  let i = 0
  for (const subdir of list) {
    if (verbose) console.log(`Upgrading ${i + 1}/${list.length}: ${subdir}`)

    try {
      await upgrade({
        filename: join(dir, subdir, 'ept.json'),
        threads,
        verbose,
      })
    } catch (e) {
      if (verbose) console.log(`Error during ${subdir}: ${e.message}`)
    }

    ++i
  }
}

type Upgrade = {
  filename: string
  threads?: number
  verbose?: boolean
}
export async function upgrade({
  filename,
  threads = 8,
  verbose = true,
}: Upgrade) {
  if (!filename.endsWith('ept.json')) {
    throw new Error('Filename must end with "ept.json"')
  }
  const dir = join(filename, '..')
  await maybeBackup({ dir, threads, verbose })

  const backup = join(dir, 'ept-backup')
  if (verbose) console.log('Getting EPT...')
  const eptjson = await Forager.readJson(join(backup, 'ept.json'))

  // A few extremely old datasets have this issue.
  if (!eptjson.points && eptjson.numPoints) {
    eptjson.points = eptjson.numPoints
    delete eptjson.numPoints
  }
  const [ept, errors] = JsonSchema.validate<Ept>(Ept.schema, eptjson)
  if (errors.length) {
    if (verbose) errors.forEach((e) => console.log(`! ${e}`))
    throw new Error('Invalid EPT')
  }

  if (verbose) console.log('\tDone')

  // Unfortunately we've got some data generated by an unreleased version of
  // Entwine - which generated ept-sources/ in version 1.0.1 format but wrote
  // their EPT version as 1.0.0.  So we can't trust the version of the backed up
  // metadata.  Regardless, we're always going to output version 1.0.1
  ept.version = '1.0.1'

  if (verbose) console.log('Awakening source file metadata...')
  const [summary, detail] = await awakenFromV0(
    join(backup, 'ept-sources'),
    verbose
  )
  if (verbose) console.log('\tDone')

  if (verbose) console.log('Updating metadata...')
  await Pool.all(
    detail.map((v, i) => async () => {
      const filename = summary[i].metadataPath
      return Forager.write(
        join(dir, 'ept-sources', filename),
        JSON.stringify(v, null, 2)
      )
    }),
    threads
  )
  await Forager.write(
    join(dir, 'ept-sources/manifest.json'),
    JSON.stringify(summary, null, 2)
  )
  await Forager.write(join(dir, 'ept.json'), JSON.stringify(ept, null, 2))

  if (verbose) {
    console.log('\tDone')
    console.log('Upgraded to EPT version 1.0.1')
  }
}

async function awakenFromV0(dir: string, verbose = true) {
  const [summary, errors] = JsonSchema.validate<Source.V0.Summary>(
    Source.V0.summary.schema,
    await Forager.readJson(join(dir, 'list.json'))
  )
  if (errors.length) {
    if (verbose) errors.forEach((e) => console.log(e))
    throw new Error('Invalid source file metadata v1.0.0')
  }

  const chunkFilenames = [
    ...summary.reduce<Set<string>>((set, item) => {
      set.add(item.url)
      return set
    }, new Set<string>()),
  ]

  // Aggregate all of the detail objects into a single object.
  const details = (
    await Pool.all(
      chunkFilenames.map((filename) => async () => {
        const [chunk, errors] = JsonSchema.validate<Source.V0.Detail>(
          Source.V0.detail.schema,
          await Forager.readJson(join(dir, filename))
        )
        if (errors.length) {
          if (verbose) errors.forEach((e) => console.log(e))
          throw new Error(`Failed validation: ${filename}`)
        }
        return chunk
      })
    )
  ).reduce<Source.V0.Detail>((agg, cur) => ({ ...agg, ...cur }), {})

  return upgradeFromV0(summary, details)
}

function upgradeFromV0(
  oldsummary: Source.V0.Summary,
  olddetail: Source.V0.Detail
): [Source.Summary, Source.Detail[]] {
  const summary = oldsummary.map<Source.Summary.Item>((v) => ({
    bounds: v.bounds,
    inserted: v.status === 'inserted' || v.status === 'error',
    // The metadata path is tentative - if the basenames are not unique, we'll
    // have to update these later.
    metadataPath: `${parse(v.path).name}.json`,
    path: v.path,
    points: v.points,
  }))

  // If our metadata paths, computed from the input filenames, are not unique,
  // then we'll just call them 0.json ... N.json.
  const namesUnique =
    new Set<string>([...summary.map((v) => v.metadataPath)]).size ===
    summary.length
  if (!namesUnique) summary.forEach((v, i) => (v.metadataPath = `${i}.json`))

  const detail: Source.Detail[] = summary.map((v, i) => {
    const id = oldsummary[i].id
    const from = olddetail[id]
    return {
      bounds: v.bounds,
      path: v.path,
      points: v.points,
      metadata: from?.metadata,
      srs: from.srs,
    }
  })

  return [summary, detail]
}
